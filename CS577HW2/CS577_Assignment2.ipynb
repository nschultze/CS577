{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_hx8SJJR4-"
      },
      "source": [
        "# Assignment 2 - Recurrent Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AiWTVDf7cZ2"
      },
      "source": [
        "## Programming (Full points: 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this assignment, our goal is to use PyTorch to implement Recurrent Neural Networks (RNN) for sentiment analysis task. Sentiment analysis is to classify sentences (input) into certain sentiments (output labels), which includes positive, negative and neutral.\n",
        "\n",
        "We will use a benckmark dataset, SST, for this assignment.\n",
        "* we download the SST dataset from torchtext package, and do some preprocessing to build vocabulary and split the dataset into training/validation/test sets. You don't need to modify the code in this step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "# load data splits\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "# build dictionary\n",
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "\n",
        "# build iterators\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data), \n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* define the training and evaluation function in the cell below.\n",
        "### (25 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n",
        "\n",
        "# Define the training function\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for batch in iterator:\n",
        "        text, labels = batch.text, batch.label\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(text)\n",
        "        \n",
        "        # Flatten predictions and labels to match shapes for loss calculation\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        labels = labels.view(-1)\n",
        "        \n",
        "        loss = criterion(predictions, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# Define the evaluation function\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, labels = batch.text, batch.label\n",
        "            predictions = model(text)\n",
        "            \n",
        "            # Flatten predictions and labels to match shapes for loss calculation\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            labels = labels.view(-1)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* build a RNN model for sentiment analysis in the cell below.\n",
        "We have provided several hyperparameters we needed for building the model, including vocabulary size (vocab_size), the word embedding dimension (embedding_dim), the hidden layer dimension (hidden_dim), the number of layers (num_layers) and the number of sentence labels (label_size). Please fill in the missing codes, and implement a RNN model.\n",
        "### (40 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = 1  # Number of RNN layers\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(self.hidden_dim, self.label_size)\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        # Initialize the hidden state with zeros\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, sequence_length]\n",
        "\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text)  # embedded: [batch_size, sequence_length, embedding_dim]\n",
        "\n",
        "        # Initialize the hidden state\n",
        "        hidden = self.zero_state(text.size(0)).to(text.device)\n",
        "\n",
        "        # RNN layer\n",
        "        rnn_output, _ = self.rnn(embedded, hidden)\n",
        "        # rnn_output: [batch_size, sequence_length, hidden_dim]\n",
        "\n",
        "        # Get the final output for classification (using the last time step)\n",
        "        final_output = rnn_output[:, -1, :]  # final_output: [batch_size, hidden_dim]\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        predictions = self.fc(final_output)  # predictions: [batch_size, label_size]\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* train the model and compute the accuracy in the cell below.\n",
        "### (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy calculation function\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    # Convert predictions to class labels (argmax)\n",
        "    predicted_labels = torch.argmax(predictions, dim=1)\n",
        "    # Compare with ground truth labels\n",
        "    correct = (predicted_labels == labels).float()\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.0540 - Accuracy: 0.4182\n",
            "Epoch [2/10] - Loss: 1.0495 - Accuracy: 0.4264\n",
            "Epoch [3/10] - Loss: 1.0463 - Accuracy: 0.4213\n",
            "Epoch [4/10] - Loss: 1.0454 - Accuracy: 0.4243\n",
            "Epoch [5/10] - Loss: 1.0457 - Accuracy: 0.4283\n",
            "Epoch [6/10] - Loss: 1.0414 - Accuracy: 0.4216\n",
            "Epoch [7/10] - Loss: 1.0415 - Accuracy: 0.4293\n",
            "Epoch [8/10] - Loss: 1.0375 - Accuracy: 0.4316\n",
            "Epoch [9/10] - Loss: 1.0372 - Accuracy: 0.4271\n",
            "Epoch [10/10] - Loss: 1.0343 - Accuracy: 0.4346\n",
            "Test Accuracy: 42.63%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize lists to store predictions and ground truth labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    # Initialize variables to store accuracy\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Calculate accuracy on the training set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for batch in train_iter:\n",
        "            text, labels = batch.text, batch.label\n",
        "            predictions = model(text)\n",
        "            all_predictions.append(predictions)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "            # Calculate accuracy for this batch\n",
        "            train_correct += torch.sum(torch.argmax(predictions, dim=1) == labels).item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    model.train()  # Set the model back to training mode\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {train_loss:.4f} - Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# Concatenate the predictions and labels into single tensors\n",
        "all_predictions = torch.cat(all_predictions)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "# Calculate the test accuracy using the calculate_accuracy function\n",
        "test_accuracy = calculate_accuracy(all_predictions, all_labels)\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* try to train a model with better accuracy in the cell below. For example, you can use different optimizers such as SGD and Adam. You can also compare different hyperparameters and model size.\n",
        "### (15 points), to obtain FULL point in this problem, the accuracy needs to be higher than 70%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I was not able to get my accuracy higher than 70%. I have included several methods that I tried to get my accuracy above 70%, however none of them were able to succeed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying with 5 Epochs instead of 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - Loss: 1.0527 - Accuracy: 0.4196\n",
            "Epoch [2/5] - Loss: 1.0486 - Accuracy: 0.4217\n",
            "Epoch [3/5] - Loss: 1.0480 - Accuracy: 0.4225\n",
            "Epoch [4/5] - Loss: 1.0458 - Accuracy: 0.4266\n",
            "Epoch [5/5] - Loss: 1.0424 - Accuracy: 0.4273\n",
            "Test Accuracy: 42.35%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize lists to store predictions and ground truth labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    # Initialize variables to store accuracy\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Calculate accuracy on the training set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for batch in train_iter:\n",
        "            text, labels = batch.text, batch.label\n",
        "            predictions = model(text)\n",
        "            all_predictions.append(predictions)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "            # Calculate accuracy for this batch\n",
        "            train_correct += torch.sum(torch.argmax(predictions, dim=1) == labels).item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    model.train()  # Set the model back to training mode\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {train_loss:.4f} - Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# Concatenate the predictions and labels into single tensors\n",
        "all_predictions = torch.cat(all_predictions)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "# Calculate the test accuracy using the calculate_accuracy function\n",
        "test_accuracy = calculate_accuracy(all_predictions, all_labels)\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying with 3 Epochs instead of 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3] - Loss: 1.0524 - Accuracy: 0.3958\n",
            "Epoch [2/3] - Loss: 1.0490 - Accuracy: 0.4254\n",
            "Epoch [3/3] - Loss: 1.0478 - Accuracy: 0.4216\n",
            "Test Accuracy: 41.43%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize lists to store predictions and ground truth labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    # Initialize variables to store accuracy\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Calculate accuracy on the training set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for batch in train_iter:\n",
        "            text, labels = batch.text, batch.label\n",
        "            predictions = model(text)\n",
        "            all_predictions.append(predictions)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "            # Calculate accuracy for this batch\n",
        "            train_correct += torch.sum(torch.argmax(predictions, dim=1) == labels).item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    model.train()  # Set the model back to training mode\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {train_loss:.4f} - Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# Concatenate the predictions and labels into single tensors\n",
        "all_predictions = torch.cat(all_predictions)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "# Calculate the test accuracy using the calculate_accuracy function\n",
        "test_accuracy = calculate_accuracy(all_predictions, all_labels)\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying with 15 Epochs instead of 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15] - Loss: 1.0523 - Accuracy: 0.4198\n",
            "Epoch [2/15] - Loss: 1.0482 - Accuracy: 0.4264\n",
            "Epoch [3/15] - Loss: 1.0460 - Accuracy: 0.4230\n",
            "Epoch [4/15] - Loss: 1.0660 - Accuracy: 0.4088\n",
            "Epoch [5/15] - Loss: 1.0549 - Accuracy: 0.4074\n",
            "Epoch [6/15] - Loss: 1.0564 - Accuracy: 0.4235\n",
            "Epoch [7/15] - Loss: 1.0543 - Accuracy: 0.4249\n",
            "Epoch [8/15] - Loss: 1.0503 - Accuracy: 0.4118\n",
            "Epoch [9/15] - Loss: 1.0492 - Accuracy: 0.4304\n",
            "Epoch [10/15] - Loss: 1.0471 - Accuracy: 0.4267\n",
            "Epoch [11/15] - Loss: 1.0465 - Accuracy: 0.4082\n",
            "Epoch [12/15] - Loss: 1.0545 - Accuracy: 0.4144\n",
            "Epoch [13/15] - Loss: 1.0548 - Accuracy: 0.4089\n",
            "Epoch [14/15] - Loss: 1.0572 - Accuracy: 0.4011\n",
            "Epoch [15/15] - Loss: 1.0564 - Accuracy: 0.4144\n",
            "Test Accuracy: 41.67%\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Initialize lists to store predictions and ground truth labels\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 15\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_iter, optimizer, criterion)\n",
        "    valid_loss = evaluate(model, val_iter, criterion)\n",
        "\n",
        "    # Initialize variables to store accuracy\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Calculate accuracy on the training set\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for batch in train_iter:\n",
        "            text, labels = batch.text, batch.label\n",
        "            predictions = model(text)\n",
        "            all_predictions.append(predictions)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "            # Calculate accuracy for this batch\n",
        "            train_correct += torch.sum(torch.argmax(predictions, dim=1) == labels).item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    model.train()  # Set the model back to training mode\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {train_loss:.4f} - Accuracy: {train_accuracy:.4f}')\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(best_model)\n",
        "\n",
        "# Concatenate the predictions and labels into single tensors\n",
        "all_predictions = torch.cat(all_predictions)\n",
        "all_labels = torch.cat(all_labels)\n",
        "\n",
        "# Calculate the test accuracy using the calculate_accuracy function\n",
        "test_accuracy = calculate_accuracy(all_predictions, all_labels)\n",
        "\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layer with no dropout and Bidirectionality with 10 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Define the RNN model class\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=1, dropout=0.0):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Embedding layer with pretrained word vectors\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(2 * self.hidden_dim, self.label_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, sequence_length]\n",
        "\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # RNN layer\n",
        "        rnn_output, _ = self.rnn(embedded)\n",
        "\n",
        "        # Get the final output for classification (using the last time step)\n",
        "        final_output = rnn_output[:, -1, :]\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        predictions = self.fc(final_output)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.0504 - Accuracy: 0.4137\n",
            "Epoch [2/10] - Loss: 1.0464 - Accuracy: 0.4147\n",
            "Epoch [3/10] - Loss: 1.0408 - Accuracy: 0.4274\n",
            "Epoch [4/10] - Loss: 1.0052 - Accuracy: 0.5094\n",
            "Epoch [5/10] - Loss: 0.9103 - Accuracy: 0.6024\n",
            "Epoch [6/10] - Loss: 0.7797 - Accuracy: 0.6833\n",
            "Epoch [7/10] - Loss: 0.6513 - Accuracy: 0.7353\n",
            "Epoch [8/10] - Loss: 0.5259 - Accuracy: 0.7968\n",
            "Epoch [9/10] - Loss: 0.4393 - Accuracy: 0.8414\n",
            "Epoch [10/10] - Loss: 0.3553 - Accuracy: 0.8810\n",
            "Test Accuracy: 55.67%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the model with pretrained embeddings and num_layers as 2\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=2, dropout=0.2)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layer with no dropout and Bidirectionality with 5 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - Loss: 1.0495 - Accuracy: 0.4170\n",
            "Epoch [2/5] - Loss: 1.0479 - Accuracy: 0.4192\n",
            "Epoch [3/5] - Loss: 1.0411 - Accuracy: 0.4315\n",
            "Epoch [4/5] - Loss: 0.9919 - Accuracy: 0.5253\n",
            "Epoch [5/5] - Loss: 0.9048 - Accuracy: 0.6025\n",
            "Test Accuracy: 50.45%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 5\n",
        "\n",
        "# Initialize the model with pretrained embeddings and num_layers as 2\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=2, dropout=0.2)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layer with no dropout and Bidirectionality with 3 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3] - Loss: 1.0494 - Accuracy: 0.4153\n",
            "Epoch [2/3] - Loss: 1.0478 - Accuracy: 0.4183\n",
            "Epoch [3/3] - Loss: 1.0439 - Accuracy: 0.4119\n",
            "Test Accuracy: 41.21%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 3\n",
        "\n",
        "# Initialize the model with pretrained embeddings and num_layers as 2\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=2, dropout=0.2)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layer with no dropout and Bidirectionality with 15 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15] - Loss: 1.0503 - Accuracy: 0.4136\n",
            "Epoch [2/15] - Loss: 1.0477 - Accuracy: 0.4173\n",
            "Epoch [3/15] - Loss: 1.0439 - Accuracy: 0.4205\n",
            "Epoch [4/15] - Loss: 1.0337 - Accuracy: 0.4428\n",
            "Epoch [5/15] - Loss: 0.9759 - Accuracy: 0.5439\n",
            "Epoch [6/15] - Loss: 0.8430 - Accuracy: 0.6489\n",
            "Epoch [7/15] - Loss: 0.6912 - Accuracy: 0.7206\n",
            "Epoch [8/15] - Loss: 0.5656 - Accuracy: 0.7789\n",
            "Epoch [9/15] - Loss: 0.4588 - Accuracy: 0.8294\n",
            "Epoch [10/15] - Loss: 0.3705 - Accuracy: 0.8723\n",
            "Epoch [11/15] - Loss: 0.3002 - Accuracy: 0.9038\n",
            "Epoch [12/15] - Loss: 0.2473 - Accuracy: 0.9258\n",
            "Epoch [13/15] - Loss: 0.2077 - Accuracy: 0.9423\n",
            "Epoch [14/15] - Loss: 0.1831 - Accuracy: 0.9496\n",
            "Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [15/15] - Loss: 0.1315 - Accuracy: 0.9652\n",
            "Test Accuracy: 56.52%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 15\n",
        "\n",
        "# Initialize the model with pretrained embeddings and num_layers as 2\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, num_layers=2, dropout=0.2)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Glove Embeddings with 10 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the RNN model class\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, pretrained_embeddings=None, num_layers=1, dropout=0.0):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Embedding layer with pretrained word vectors\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=padding_idx, freeze=False)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # LSTM layer with bidirectionality\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(2 * self.hidden_dim, self.label_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, sequence_length]\n",
        "\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # LSTM layer\n",
        "        rnn_output, _ = self.rnn(embedded)\n",
        "\n",
        "        # Apply dropout\n",
        "        rnn_output = self.dropout(rnn_output)\n",
        "\n",
        "        # Get the final output for classification (using the last time step)\n",
        "        final_output = rnn_output[:, -1, :]\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        predictions = self.fc(final_output)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.0525 - Accuracy: 0.4150\n",
            "Epoch [2/10] - Loss: 1.0514 - Accuracy: 0.4081\n",
            "Epoch [3/10] - Loss: 1.0494 - Accuracy: 0.4105\n",
            "Epoch [4/10] - Loss: 1.0491 - Accuracy: 0.4139\n",
            "Epoch 00004: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [5/10] - Loss: 1.0474 - Accuracy: 0.4198\n",
            "Epoch [6/10] - Loss: 1.0459 - Accuracy: 0.4188\n",
            "Epoch [7/10] - Loss: 1.0358 - Accuracy: 0.4278\n",
            "Epoch 00007: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch [8/10] - Loss: 0.9810 - Accuracy: 0.5176\n",
            "Epoch [9/10] - Loss: 0.8956 - Accuracy: 0.5897\n",
            "Epoch [10/10] - Loss: 0.8087 - Accuracy: 0.6428\n",
            "Test Accuracy: 56.61%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the model with pretrained embeddings, more layers, and dropout\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,\n",
        "                      dropout=0.5)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "clip_value = 1.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Learning rate scheduling based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Glove Embeddings with 5 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - Loss: 1.0543 - Accuracy: 0.4108\n",
            "Epoch [2/5] - Loss: 1.0495 - Accuracy: 0.4153\n",
            "Epoch [3/5] - Loss: 1.0496 - Accuracy: 0.4160\n",
            "Epoch [4/5] - Loss: 1.0498 - Accuracy: 0.4148\n",
            "Epoch [5/5] - Loss: 1.0491 - Accuracy: 0.4171\n",
            "Test Accuracy: 41.25%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 5\n",
        "\n",
        "# Initialize the model with pretrained embeddings, more layers, and dropout\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,\n",
        "                      dropout=0.5)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "clip_value = 1.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Learning rate scheduling based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Glove Embeddings with 3 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3] - Loss: 1.0518 - Accuracy: 0.4187\n",
            "Epoch [2/3] - Loss: 1.0500 - Accuracy: 0.4121\n",
            "Epoch [3/3] - Loss: 1.0494 - Accuracy: 0.4106\n",
            "Test Accuracy: 41.25%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 3\n",
        "\n",
        "# Initialize the model with pretrained embeddings, more layers, and dropout\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,\n",
        "                      dropout=0.5)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "clip_value = 1.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Learning rate scheduling based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Glove Embeddings with 15 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15] - Loss: 1.0532 - Accuracy: 0.4086\n",
            "Epoch [2/15] - Loss: 1.0506 - Accuracy: 0.4177\n",
            "Epoch [3/15] - Loss: 1.0499 - Accuracy: 0.4177\n",
            "Epoch [4/15] - Loss: 1.0488 - Accuracy: 0.4221\n",
            "Epoch [5/15] - Loss: 1.0487 - Accuracy: 0.4158\n",
            "Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [6/15] - Loss: 1.0414 - Accuracy: 0.4283\n",
            "Epoch [7/15] - Loss: 0.9242 - Accuracy: 0.5837\n",
            "Epoch [8/15] - Loss: 0.7477 - Accuracy: 0.6821\n",
            "Epoch 00008: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch [9/15] - Loss: 0.6179 - Accuracy: 0.7362\n",
            "Epoch [10/15] - Loss: 0.5606 - Accuracy: 0.7534\n",
            "Epoch [11/15] - Loss: 0.5096 - Accuracy: 0.7676\n",
            "Epoch [12/15] - Loss: 0.4652 - Accuracy: 0.7945\n",
            "Epoch [13/15] - Loss: 0.4188 - Accuracy: 0.8256\n",
            "Epoch [14/15] - Loss: 0.3721 - Accuracy: 0.8522\n",
            "Epoch [15/15] - Loss: 0.3337 - Accuracy: 0.8756\n",
            "Test Accuracy: 49.11%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 15\n",
        "\n",
        "# Initialize the model with pretrained embeddings, more layers, and dropout\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,\n",
        "                      dropout=0.5)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "clip_value = 1.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Learning rate scheduling based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Embeddings with 10 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the RNN model class\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, pretrained_embeddings=None):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = 1\n",
        "\n",
        "        # Embedding layer with pretrained word vectors\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=padding_idx)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(2 * self.hidden_dim, self.label_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, sequence_length]\n",
        "\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        rnn_output, _ = self.rnn(embedded)\n",
        "\n",
        "        # Get the final output for classification (using the last time step)\n",
        "        final_output = rnn_output[:, -1, :]\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        predictions = self.fc(final_output)\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.0523 - Accuracy: 0.4143\n",
            "Epoch [2/10] - Loss: 1.0492 - Accuracy: 0.4127\n",
            "Epoch [3/10] - Loss: 1.0482 - Accuracy: 0.4235\n",
            "Epoch [4/10] - Loss: 1.0477 - Accuracy: 0.4198\n",
            "Epoch [5/10] - Loss: 1.0476 - Accuracy: 0.4202\n",
            "Epoch [6/10] - Loss: 1.0469 - Accuracy: 0.4144\n",
            "Epoch [7/10] - Loss: 1.0468 - Accuracy: 0.4224\n",
            "Epoch [8/10] - Loss: 1.0458 - Accuracy: 0.4222\n",
            "Epoch [9/10] - Loss: 1.0452 - Accuracy: 0.4243\n",
            "Epoch [10/10] - Loss: 1.0431 - Accuracy: 0.4387\n",
            "Test Accuracy: 43.97%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        accuracy = (predicted_labels == labels).float().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        test_accuracy = (predicted_labels == labels).float().mean()\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Embeddings with 5 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - Loss: 1.0509 - Accuracy: 0.4151\n",
            "Epoch [2/5] - Loss: 1.0486 - Accuracy: 0.4168\n",
            "Epoch [3/5] - Loss: 1.0478 - Accuracy: 0.4189\n",
            "Epoch [4/5] - Loss: 1.0487 - Accuracy: 0.4197\n",
            "Epoch [5/5] - Loss: 1.0482 - Accuracy: 0.4194\n",
            "Test Accuracy: 41.29%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 5\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        accuracy = (predicted_labels == labels).float().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        test_accuracy = (predicted_labels == labels).float().mean()\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Embeddings with 3 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3] - Loss: 1.0518 - Accuracy: 0.4121\n",
            "Epoch [2/3] - Loss: 1.0488 - Accuracy: 0.4149\n",
            "Epoch [3/3] - Loss: 1.0490 - Accuracy: 0.4165\n",
            "Test Accuracy: 41.25%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 3\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        accuracy = (predicted_labels == labels).float().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        test_accuracy = (predicted_labels == labels).float().mean()\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 2 LSTM Layers with dropout, Bidirectionality, and Pretrained Embeddings with 15 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15] - Loss: 1.0512 - Accuracy: 0.4133\n",
            "Epoch [2/15] - Loss: 1.0489 - Accuracy: 0.4205\n",
            "Epoch [3/15] - Loss: 1.0481 - Accuracy: 0.4226\n",
            "Epoch [4/15] - Loss: 1.0478 - Accuracy: 0.4189\n",
            "Epoch [5/15] - Loss: 1.0478 - Accuracy: 0.4188\n",
            "Epoch [6/15] - Loss: 1.0474 - Accuracy: 0.4225\n",
            "Epoch [7/15] - Loss: 1.0461 - Accuracy: 0.4217\n",
            "Epoch [8/15] - Loss: 1.0471 - Accuracy: 0.4202\n",
            "Epoch [9/15] - Loss: 1.0458 - Accuracy: 0.4181\n",
            "Epoch [10/15] - Loss: 1.0458 - Accuracy: 0.4204\n",
            "Epoch [11/15] - Loss: 1.0446 - Accuracy: 0.4239\n",
            "Epoch [12/15] - Loss: 1.0427 - Accuracy: 0.4208\n",
            "Epoch [13/15] - Loss: 1.0420 - Accuracy: 0.4272\n",
            "Epoch [14/15] - Loss: 1.0406 - Accuracy: 0.4233\n",
            "Epoch [15/15] - Loss: 1.0389 - Accuracy: 0.4239\n",
            "Test Accuracy: 34.91%\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 15\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        accuracy = (predicted_labels == labels).float().mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        _, predicted_labels = torch.max(predictions, 1)\n",
        "        test_accuracy = (predicted_labels == labels).float().mean()\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layers with dropout, Bidirectionality, Pretrained Glove Embeddings, and Learning Rate Scheduler with 10 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the RNN model class\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, pretrained_embeddings=None, num_layers=1, dropout=0.0):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = num_layers  # Number of RNN layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Embedding layer with pretrained word vectors\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=padding_idx)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,  # Bidirectional RNN\n",
        "            dropout=self.dropout  # Apply dropout to RNN layers\n",
        "        )\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(2 * self.hidden_dim, self.label_size)  # Multiply hidden_dim by 2 due to bidirectionality\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, sequence_length]\n",
        "\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text)  # embedded: [batch_size, sequence_length, embedding_dim]\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        rnn_output, _ = self.rnn(embedded)\n",
        "        # rnn_output: [batch_size, sequence_length, 2 * hidden_dim]\n",
        "\n",
        "        # Get the final output for classification (using the last time step)\n",
        "        final_output = rnn_output[:, -1, :]  # final_output: [batch_size, 2 * hidden_dim]\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        predictions = self.fc(final_output)  # predictions: [batch_size, label_size]\n",
        "\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.0507 - Accuracy: 0.4158\n",
            "Epoch [2/10] - Loss: 1.0491 - Accuracy: 0.4183\n",
            "Epoch [3/10] - Loss: 1.0478 - Accuracy: 0.4199\n",
            "Epoch [4/10] - Loss: 1.0486 - Accuracy: 0.4187\n",
            "Epoch [5/10] - Loss: 1.0478 - Accuracy: 0.4222\n",
            "Epoch [6/10] - Loss: 1.0473 - Accuracy: 0.4177\n",
            "Epoch [7/10] - Loss: 1.0470 - Accuracy: 0.4225\n",
            "Epoch [8/10] - Loss: 1.0478 - Accuracy: 0.4190\n",
            "Epoch [9/10] - Loss: 1.0474 - Accuracy: 0.4225\n",
            "Epoch [10/10] - Loss: 1.0465 - Accuracy: 0.4229\n",
            "Test Accuracy: 0.4125\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100  # Match the GloVe embedding dimension\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,  # Experiment with the number of layers\n",
        "                      dropout=0.2)  # Experiment with different dropout rates\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0  # Experiment with the clipping threshold\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layers with dropout, Bidirectionality, Pretrained Glove Embeddings, and Learning Rate Scheduler with 5 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] - Loss: 1.0505 - Accuracy: 0.4189\n",
            "Epoch [2/5] - Loss: 1.0495 - Accuracy: 0.4157\n",
            "Epoch [3/5] - Loss: 1.0482 - Accuracy: 0.4163\n",
            "Epoch [4/5] - Loss: 1.0486 - Accuracy: 0.4192\n",
            "Epoch 00004: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [5/5] - Loss: 1.0476 - Accuracy: 0.4225\n",
            "Test Accuracy: 0.4125\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100  # Match the GloVe embedding dimension\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 5\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,  # Experiment with the number of layers\n",
        "                      dropout=0.2)  # Experiment with different dropout rates\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0  # Experiment with the clipping threshold\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layers with dropout, Bidirectionality, Pretrained Glove Embeddings, and Learning Rate Scheduler with 3 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/3] - Loss: 1.0522 - Accuracy: 0.4142\n",
            "Epoch [2/3] - Loss: 1.0481 - Accuracy: 0.4149\n",
            "Epoch [3/3] - Loss: 1.0485 - Accuracy: 0.4134\n",
            "Test Accuracy: 0.4125\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100  # Match the GloVe embedding dimension\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 3\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,  # Experiment with the number of layers\n",
        "                      dropout=0.2)  # Experiment with different dropout rates\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0  # Experiment with the clipping threshold\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with 1 LSTM Layers with dropout, Bidirectionality, Pretrained Glove Embeddings, and Learning Rate Scheduler with 15 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15] - Loss: 1.0502 - Accuracy: 0.4147\n",
            "Epoch [2/15] - Loss: 1.0487 - Accuracy: 0.4182\n",
            "Epoch [3/15] - Loss: 1.0484 - Accuracy: 0.4174\n",
            "Epoch [4/15] - Loss: 1.0482 - Accuracy: 0.4147\n",
            "Epoch [5/15] - Loss: 1.0484 - Accuracy: 0.4201\n",
            "Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [6/15] - Loss: 1.0477 - Accuracy: 0.4225\n",
            "Epoch [7/15] - Loss: 1.0468 - Accuracy: 0.4225\n",
            "Epoch [8/15] - Loss: 1.0473 - Accuracy: 0.4225\n",
            "Epoch 00008: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch [9/15] - Loss: 1.0463 - Accuracy: 0.4225\n",
            "Epoch [10/15] - Loss: 1.0459 - Accuracy: 0.4225\n",
            "Epoch [11/15] - Loss: 1.0458 - Accuracy: 0.4225\n",
            "Epoch [12/15] - Loss: 1.0456 - Accuracy: 0.4225\n",
            "Epoch [13/15] - Loss: 1.0449 - Accuracy: 0.4225\n",
            "Epoch 00013: reducing learning rate of group 0 to 1.2500e-04.\n",
            "Epoch [14/15] - Loss: 1.0450 - Accuracy: 0.4225\n",
            "Epoch [15/15] - Loss: 1.0449 - Accuracy: 0.4225\n",
            "Test Accuracy: 0.4125\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "embedding_dim = 100  # Match the GloVe embedding dimension\n",
        "hidden_dim = 128\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "num_epochs = 15\n",
        "\n",
        "# Initialize the model with pretrained embeddings\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                      num_layers=2,  # Experiment with the number of layers\n",
        "                      dropout=0.2)  # Experiment with different dropout rates\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)  # Learning rate scheduler\n",
        "\n",
        "# Training loop with gradient clipping\n",
        "clip_value = 1.0  # Experiment with the clipping threshold\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for batch in train_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(text)\n",
        "        loss = criterion(predictions, labels)\n",
        "        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_iter)\n",
        "    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "    # Adjust learning rate using the scheduler based on validation performance\n",
        "    model.eval()\n",
        "    val_loss = evaluate(model, val_iter, criterion)\n",
        "    scheduler.step(val_loss)  # Adjust learning rate\n",
        "\n",
        "# After training, evaluate on the test set\n",
        "model.eval()\n",
        "total_test_accuracy = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        text, labels = batch.text, batch.label\n",
        "        predictions = model(text)\n",
        "        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "print(f'Test Accuracy: {average_test_accuracy:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trained with a Grid Search over embedding_dim, hidden_dim, num_layers, and dropout and Learning Rate Scheduler with 10 Epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with: embedding_dim=100, hidden_dim=128, num_layers=1, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.0504 - Accuracy: 0.4058\n",
            "Epoch [2/10] - Loss: 1.0484 - Accuracy: 0.4158\n",
            "Epoch [3/10] - Loss: 1.0488 - Accuracy: 0.4177\n",
            "Epoch [4/10] - Loss: 1.0483 - Accuracy: 0.4189\n",
            "Epoch [5/10] - Loss: 1.0477 - Accuracy: 0.4194\n",
            "Epoch [6/10] - Loss: 1.0475 - Accuracy: 0.4183\n",
            "Epoch [7/10] - Loss: 1.0472 - Accuracy: 0.4191\n",
            "Epoch [8/10] - Loss: 1.0468 - Accuracy: 0.4189\n",
            "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [9/10] - Loss: 1.0449 - Accuracy: 0.4232\n",
            "Epoch [10/10] - Loss: 1.0440 - Accuracy: 0.4218\n",
            "Test Accuracy: 0.3857\n",
            "Training with: embedding_dim=100, hidden_dim=128, num_layers=1, dropout=0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] - Loss: 1.0496 - Accuracy: 0.4247\n",
            "Epoch [2/10] - Loss: 1.0486 - Accuracy: 0.4163\n",
            "Epoch [3/10] - Loss: 1.0484 - Accuracy: 0.4178\n",
            "Epoch [4/10] - Loss: 1.0480 - Accuracy: 0.4136\n",
            "Epoch 00004: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [5/10] - Loss: 1.0474 - Accuracy: 0.4226\n",
            "Epoch [6/10] - Loss: 1.0471 - Accuracy: 0.4226\n",
            "Epoch [7/10] - Loss: 1.0467 - Accuracy: 0.4215\n",
            "Epoch 00007: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch [8/10] - Loss: 1.0459 - Accuracy: 0.4197\n",
            "Epoch [9/10] - Loss: 1.0453 - Accuracy: 0.4256\n",
            "Epoch [10/10] - Loss: 1.0429 - Accuracy: 0.4354\n",
            "Test Accuracy: 0.4830\n",
            "Training with: embedding_dim=100, hidden_dim=128, num_layers=2, dropout=0.2\n",
            "Epoch [1/10] - Loss: 1.0508 - Accuracy: 0.4146\n",
            "Epoch [2/10] - Loss: 1.0492 - Accuracy: 0.4181\n",
            "Epoch [3/10] - Loss: 1.0484 - Accuracy: 0.4226\n",
            "Epoch [4/10] - Loss: 1.0479 - Accuracy: 0.4190\n",
            "Epoch [5/10] - Loss: 1.0482 - Accuracy: 0.4212\n",
            "Epoch [6/10] - Loss: 1.0473 - Accuracy: 0.4183\n",
            "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [7/10] - Loss: 1.0471 - Accuracy: 0.4225\n",
            "Epoch [8/10] - Loss: 1.0469 - Accuracy: 0.4225\n",
            "Epoch [9/10] - Loss: 1.0466 - Accuracy: 0.4225\n",
            "Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n",
            "Epoch [10/10] - Loss: 1.0448 - Accuracy: 0.4225\n",
            "Test Accuracy: 0.4125\n",
            "Training with: embedding_dim=100, hidden_dim=128, num_layers=2, dropout=0.5\n",
            "Epoch [1/10] - Loss: 1.0515 - Accuracy: 0.4074\n",
            "Epoch [2/10] - Loss: 1.0494 - Accuracy: 0.4192\n",
            "Epoch [3/10] - Loss: 1.0486 - Accuracy: 0.4169\n",
            "Epoch [4/10] - Loss: 1.0490 - Accuracy: 0.4203\n",
            "Epoch [5/10] - Loss: 1.0486 - Accuracy: 0.4205\n",
            "Epoch [6/10] - Loss: 1.0476 - Accuracy: 0.4225\n",
            "Epoch [7/10] - Loss: 1.0485 - Accuracy: 0.4177\n",
            "Epoch [8/10] - Loss: 1.0479 - Accuracy: 0.4199\n",
            "Epoch [9/10] - Loss: 1.0483 - Accuracy: 0.4242\n",
            "Epoch [10/10] - Loss: 1.0473 - Accuracy: 0.4198\n",
            "Test Accuracy: 0.4125\n",
            "Training with: embedding_dim=100, hidden_dim=256, num_layers=1, dropout=0.2\n",
            "Epoch [1/10] - Loss: 1.0526 - Accuracy: 0.4022\n",
            "Epoch [2/10] - Loss: 1.0493 - Accuracy: 0.4183\n",
            "Epoch [3/10] - Loss: 1.0484 - Accuracy: 0.4150\n",
            "Epoch [4/10] - Loss: 1.0477 - Accuracy: 0.4185\n",
            "Epoch [5/10] - Loss: 1.0479 - Accuracy: 0.4205\n",
            "Epoch [6/10] - Loss: 1.0474 - Accuracy: 0.4226\n",
            "Epoch [7/10] - Loss: 1.0476 - Accuracy: 0.4190\n",
            "Epoch [8/10] - Loss: 1.0466 - Accuracy: 0.4205\n",
            "Epoch [9/10] - Loss: 1.0460 - Accuracy: 0.4189\n",
            "Epoch 00009: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [10/10] - Loss: 1.0453 - Accuracy: 0.4243\n",
            "Test Accuracy: 0.3786\n",
            "Training with: embedding_dim=100, hidden_dim=256, num_layers=1, dropout=0.5\n",
            "Epoch [1/10] - Loss: 1.0511 - Accuracy: 0.4135\n",
            "Epoch [2/10] - Loss: 1.0491 - Accuracy: 0.4136\n",
            "Epoch [3/10] - Loss: 1.0486 - Accuracy: 0.4158\n",
            "Epoch [4/10] - Loss: 1.0477 - Accuracy: 0.4203\n",
            "Epoch [5/10] - Loss: 1.0480 - Accuracy: 0.4199\n",
            "Epoch [6/10] - Loss: 1.0479 - Accuracy: 0.4223\n",
            "Epoch [7/10] - Loss: 1.0468 - Accuracy: 0.4224\n",
            "Epoch [8/10] - Loss: 1.0454 - Accuracy: 0.4164\n",
            "Epoch [9/10] - Loss: 1.0459 - Accuracy: 0.4206\n",
            "Epoch [10/10] - Loss: 1.0441 - Accuracy: 0.4201\n",
            "Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Test Accuracy: 0.4643\n",
            "Training with: embedding_dim=100, hidden_dim=256, num_layers=2, dropout=0.2\n",
            "Epoch [1/10] - Loss: 1.0514 - Accuracy: 0.4148\n",
            "Epoch [2/10] - Loss: 1.0488 - Accuracy: 0.4210\n",
            "Epoch [3/10] - Loss: 1.0483 - Accuracy: 0.4222\n",
            "Epoch [4/10] - Loss: 1.0481 - Accuracy: 0.4216\n",
            "Epoch [5/10] - Loss: 1.0480 - Accuracy: 0.4225\n",
            "Epoch [6/10] - Loss: 1.0477 - Accuracy: 0.4165\n",
            "Epoch [7/10] - Loss: 1.0483 - Accuracy: 0.4223\n",
            "Epoch [8/10] - Loss: 1.0473 - Accuracy: 0.4208\n",
            "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch [9/10] - Loss: 1.0460 - Accuracy: 0.4230\n",
            "Epoch [10/10] - Loss: 1.0455 - Accuracy: 0.4215\n",
            "Test Accuracy: 0.4134\n",
            "Training with: embedding_dim=100, hidden_dim=256, num_layers=2, dropout=0.5\n",
            "Epoch [1/10] - Loss: 1.0533 - Accuracy: 0.4121\n",
            "Epoch [2/10] - Loss: 1.0502 - Accuracy: 0.4129\n",
            "Epoch [3/10] - Loss: 1.0490 - Accuracy: 0.4173\n",
            "Epoch [4/10] - Loss: 1.0488 - Accuracy: 0.4202\n",
            "Epoch [5/10] - Loss: 1.0489 - Accuracy: 0.4132\n",
            "Epoch [6/10] - Loss: 1.0485 - Accuracy: 0.4185\n",
            "Epoch [7/10] - Loss: 1.0479 - Accuracy: 0.4211\n",
            "Epoch [8/10] - Loss: 1.0477 - Accuracy: 0.4221\n",
            "Epoch [9/10] - Loss: 1.0473 - Accuracy: 0.4206\n",
            "Epoch [10/10] - Loss: 1.0478 - Accuracy: 0.4225\n",
            "Test Accuracy: 0.4125\n"
          ]
        }
      ],
      "source": [
        "# Define the RNN model class\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, pretrained_embeddings=None, num_layers=1, dropout=0.0):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Embedding layer with pretrained word vectors\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=padding_idx)\n",
        "        else:\n",
        "            self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(2 * self.hidden_dim, self.label_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, sequence_length]\n",
        "\n",
        "        # Embedding layer\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        rnn_output, _ = self.rnn(embedded)\n",
        "\n",
        "        # Get the final output for classification (using the last time step)\n",
        "        final_output = rnn_output[:, -1, :]\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        predictions = self.fc(final_output)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Load pretrained word vectors (GloVe)\n",
        "glove_vectors = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# Define hyperparameter search grid\n",
        "hyperparameters = {\n",
        "    'embedding_dim': [100],\n",
        "    'hidden_dim': [128, 256],\n",
        "    'num_layers': [1, 2],\n",
        "    'dropout': [0.2, 0.5],\n",
        "}\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_model = None\n",
        "\n",
        "for embedding_dim in hyperparameters['embedding_dim']:\n",
        "    for hidden_dim in hyperparameters['hidden_dim']:\n",
        "        for num_layers in hyperparameters['num_layers']:\n",
        "            for dropout in hyperparameters['dropout']:\n",
        "                print(f\"Training with: embedding_dim={embedding_dim}, hidden_dim={hidden_dim}, num_layers={num_layers}, dropout={dropout}\")\n",
        "                \n",
        "                # Initialize the model with pretrained embeddings\n",
        "                model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, glove_vectors.vectors,\n",
        "                                      num_layers=num_layers,\n",
        "                                      dropout=dropout)\n",
        "                \n",
        "                # Loss function and optimizer\n",
        "                criterion = nn.CrossEntropyLoss()\n",
        "                optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "                \n",
        "                # Training loop with gradient clipping\n",
        "                num_epochs = 10\n",
        "                best_valid_loss = float('inf')\n",
        "\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    total_loss = 0.0\n",
        "                    total_accuracy = 0.0\n",
        "\n",
        "                    for batch in train_iter:\n",
        "                        text, labels = batch.text, batch.label\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        predictions = model(text)\n",
        "                        loss = criterion(predictions, labels)\n",
        "                        accuracy = calculate_accuracy(predictions, labels)\n",
        "\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        total_loss += loss.item()\n",
        "                        total_accuracy += accuracy.item()\n",
        "\n",
        "                    average_loss = total_loss / len(train_iter)\n",
        "                    average_accuracy = total_accuracy / len(train_iter)\n",
        "\n",
        "                    print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {average_loss:.4f} - Accuracy: {average_accuracy:.4f}')\n",
        "\n",
        "                    scheduler.step(average_accuracy)  # Adjust learning rate based on training performance\n",
        "\n",
        "                    # Check if the current model performed better than the previous best model\n",
        "                    if average_accuracy > best_accuracy:\n",
        "                        best_accuracy = average_accuracy\n",
        "                        best_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # Evaluate the current model on the test set\n",
        "                model.eval()\n",
        "                total_test_accuracy = 0.0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for batch in test_iter:\n",
        "                        text, labels = batch.text, batch.label\n",
        "                        predictions = model(text)\n",
        "                        test_accuracy = calculate_accuracy(predictions, labels)\n",
        "                        total_test_accuracy += test_accuracy.item()\n",
        "\n",
        "                average_test_accuracy = total_test_accuracy / len(test_iter)\n",
        "                print(f'Test Accuracy: {average_test_accuracy:.4f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
